{
    "document_relevance": "You are a grader assessing relevance of a retrieved document to a user question.\n                        It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n                        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n                        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.",
    "hallucination": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n                        Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.",
    "answer_quality": "You are a grader assessing whether an answer addresses / resolves a question.\n                        Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.",
    "question_rewrite": "You are a question re-writer that converts an input question to a better version that is optimized\n                        for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.",
    "generate_answer": "You are a helpful AI assistant. Use the provided context to answer the user's question.\n                        If the answer is not in the context, admit that you don't know.",
    "fallback_no_docs": "I'm sorry, I couldn't find any relevant information to answer your question. Please try rephrasing or asking a different question.",
    "fallback_with_docs": "I found some relevant information but am having trouble processing it right now. Here's what I found:"
}